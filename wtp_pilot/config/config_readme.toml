[app]
app_intro = """
This app is an attempt to show case the key results that can be shown in this pilot.
Here we have used a mock dataset. Although we tried to make numbers realistic but please note that these numbers are synthetic.
"""

[tooltips]
upload_choice = """
* Check to load a toy dataset and see what can be done with this app.
* Uncheck to upload your own dataset.
"""
toy_dataset = """
One toy datasets are available:
* __Mock WTP__: Willingness to pay for a pretend flight
"""

select_flight = """
WTP will be calculated for the selected flight"""

select_class = """
WTP will be calculated for the selected class"""



[plots]
overview = """
This visualization displays several information:
* The blue line shows the __predictions__ made by the model on both training and validation periods.
* The blue shade around is a __80% uncertainty interval__ on these predictions,
obtained through a Monte Carlo simulation.
* The black points are the __actual values__ of the target on training period.
* The red line is the __trend__ estimated by the model,
and the vertical lines show the __changepoints__ at which this trend evolves.

You can use the slider at the bottom or the buttons at the top to focus on a specific time period.
"""
metrics = """
The following metrics can be computed to evaluate model performance:
* __Mean Absolute Percentage Error (MAPE)__: Measures the average absolute size of each error in percentage
of the truth. This metric is not ideal for low-volume forecasts,
because being off by a few units can increase the percentage error signficantly.
It can't be calculated if the true value is 0 (here samples are excluded from calculation if true value is 0).
* __Symmetric Mean Absolute Percentage Error (SMAPE)__: Slight variation of the MAPE,
it measures the average absolute size of each error in percentage of the truth summed with the forecast.
It is therefore a bit more robust to 0 values.
* __Mean Squared Error (MSE)__: Measures the average squared difference between forecasts and true values.
This metric is not ideal with noisy data,
because a very bad forecast can increase the global error signficantly as all errors are squared.
* __Root Mean Squared Error (RMSE)__: Square root of the MSE.
This metric is more robust to outliers than the MSE,
as the square root limits the impact of large errors in the global error.
* __Mean Absolute Error (MAE)__: Measures the average absolute error.
This metric can be interpreted as the absolute average distance between the best possible fit and the forecast.
"""
components = """
The forecast generated by Prophet is the sum of different contributions:
* Trend
* Seasonalities
* Other factors such as holidays or external regressors

The following visualization shows this breakdown and allows you to understand how each component contributes
to the final value forecasted by the model.
"""
waterfall = """
This plot shows the contributions of each components on a specific period of time.
All contributions are averaged over the selected period.
"""
future = """
This visualization can be read in the same way as the overview plot:
* The blue line shows the predictions made by the model for the period to be forecasted.
* The blue shade is a 80% uncertainty interval.
* The red line is the trend estimated by the model.
"""
helper_metrics = """
The following table and plots allow you to evaluate model performance. Go to the **Evaluation** section of the sidebar if you wish to customize evaluation settings by:
* Adding more metrics
* Changing evaluation period
* Computing performance at a different granularity to understand on which periods performance drops
"""
helper_errors = """
The following plots can help you to detect patterns in forecasting errors:
* The first one shows forecasts vs the ground truth on evaluation period.
* The second one helps to find the worst performing forecasts (ie points far from the red line).
* The third one shows how errors are distributed (see whether the model makes under-prediction or over-prediction errors).

If you detect a recurring error, change cleaning options or model parameters to try to correct it.
"""

[links]
repo = "https://github.com/artefactory/streamlit_prophet"
article = "https://medium.com/artefact-engineering-and-data-science/visual-time-series-forecasting-with-streamlit-prophet-71d86a769928"
